---
---

# 2024

@article{zucchetb_recurrent_2024,
	title = {Recurrent neural networks: vanishing and exploding gradients are not the end of the story},
	abstract = {Recurrent neural networks (RNNs) notoriously struggle to learn long-term memories, primarily due to vanishing and exploding gradients. The recent success of state-space models (SSMs), a subclass of RNNs, to overcome such difficulties challenges our theoretical understanding. In this paper, we delve into the optimization challenges of RNNs and discover that, as the memory of a network increases, changes in its parameters result in increasingly large output variations, making gradient-based learning highly sensitive, even without exploding gradients. Our analysis further reveals the importance of the element-wise recurrence design pattern combined with careful parametrizations in mitigating this effect. This feature is present in SSMs, as well as in other architectures, such as LSTMs. Overall, our insights provide a new explanation for some of the difficulties in gradient-based learning of RNNs and why some architectures perform better than others.},
	journal = {arXiv preprint arXiv:2405.21064},
	author = {Zucchet</b>, <b>Nicolas and Orvieto, Antonio},
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {Zucchet and Orvieto - 2024 - Recurrent neural networks vanishing and exploding.pdf:/Users/nicolas/Zotero/storage/CSKYBDYM/Zucchet and Orvieto - 2024 - Recurrent neural networks vanishing and exploding.pdf:application/pdf},
	selected={true},
}



# 2023

@article{zucchetb_gated_2023,
	title = {Gated recurrent neural networks discover attention},
	journal = {arXiv preprint arXiv:2309.01775},
	author = {Zucchet*</b>, <b>Nicolas and Kobayashi*, Seijin and Akram*, Yassir and von Oswald, Johannes and Larcher, Maxime and Steger, Angelika and Sacramento, João},
	year = {2023},
  selected={true},
  pdf={https://arxiv.org/pdf/2309.01775},
  thread={https://x.com/NicolasZucchet/status/1755925788648739276},
}

@article{von_oswald_uncovering_2023,
	title = {Uncovering mesa-optimization algorithms in transformers},
	journal = {arXiv preprint arXiv:2309.05858},
	author = {von Oswald*, Johannes and Niklasson*, Eyvind and Schlegel*, Maximilian and Kobayashi, Seijin and Zucchet</b>, <b>Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and Vladymyrov, Max and Pascanu, Razvan and Sacramento, João},
	year = {2023},
  pdf={https://arxiv.org/pdf/2309.05858},
  thread={https://x.com/oswaldjoh/status/1701873029100241241},
}

@inproceedings{zucchetb_online_2023,
	title = {Online learning of long-range dependencies},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zucchet*</b>, <b>Nicolas and Meier*, Robert and Schug*, Simon and Mujika, Asier and Sacramento, João},
	year = {2023},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  selected={true},
  code={https://github.com/NicolasZucchet/Online-learning-LR-dependencies},
  pdf={https://arxiv.org/pdf/2305.15947},
  poster={poster_online_learning_long_range_dependencies.pdf},
  thread={https://x.com/NicolasZucchet/status/1733119036140249579},
}

# 2022

@inproceedings{meulemans_least-control_2022,
	title = {The least-control principle for local learning at equilibrium},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} (Oral)},
	author = {Meulemans*, Alexander and Zucchet*</b>, <b>Nicolas and Kobayashi*, Seijin and von Oswald, Johannes and Sacramento, João},
	year = {2022},
  selected={true},
  pdf={https://arxiv.org/pdf/2207.01332},
  code={https://github.com/seijin-kobayashi/least-control},
  poster={poster_lcp.pdf},
  thread={https://x.com/NicolasZucchet/status/1591041946856062976},
}

@inproceedings{benzing_random_2022,
	title = {Random initialisations performing above chance and how to find them},
	booktitle = {Annual {Workshop} on {Optimization} for {Machine} {Learning}},
	author = {Benzing, Frederik and Schug, Simon and Meier, Robert and von Oswald, Johannes and Akram, Yassir and Zucchet</b>, <b>Nicolas and Aitchison, Laurence and Steger, Angelika},
	year = {2022},
	keywords = {Computer Science - Machine Learning},
  pdf={https://arxiv.org/pdf/2209.07509},
  thread={https://x.com/ssmonsays/status/1571126507933929481},
}

@inproceedings{zucchetb_contrastive_2022,
	title = {A contrastive rule for meta-learning},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zucchet*</b>, <b>Nicolas and Schug*, Simon and von Oswald*, Johannes and Zhao, Dominic and Sacramento, João},
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
  pdf={https://arxiv.org/pdf/2104.01677},
  poster={poster_cml.png},
  thread={https://x.com/ssmonsays/status/1585955752854048768},
}

@article{zucchetb_beyond_2022,
	title = {Beyond backpropagation: bilevel optimization through implicit differentiation and equilibrium propagation},
	volume = {34},
	number = {12},
	journal = {Neural Computation},
	author = {Zucchet</b>, <b>Nicolas and Sacramento, João},
	year = {2022},
  pdf={https://arxiv.org/pdf/2205.03076},
}

# 2021

@inproceedings{von_oswald_learning_2021,
	title = {Learning where to learn: {Gradient} sparsity in meta and continual learning},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {von Oswald*, Johannes and Zhao*, Dominic and Kobayashi, Seijin and Schug, Simon and Caccia, Massimo and Zucchet</b>, <b>Nicolas and Sacramento, João},
	year = {2021},
  pdf={https://arxiv.org/pdf/2110.14402},
}

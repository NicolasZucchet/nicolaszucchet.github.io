---
---

# 2025

@article{zucchet_facts_2025,
	title = {How language models learn facts? Dynamics, curricula and hallucinations},
	journal = {arXiv preprint arXiv:2503.21676},
	author = {Zucchet</b>, <b>Nicolas and Bornschein, Jörg and Chan, Stephanie and Lampinen, Andrew and Pascanu, Razvan and De, Soham},
	year = {2025},
	selected={true},
	pdf={https://arxiv.org/pdf/2503.21676},
}

# 2024

@article{zucchetb_recurrent_2024,
	title = {Recurrent neural networks: vanishing and exploding gradients are not the end of the story},
	journal = {arXiv preprint arXiv:2405.21064},
	author = {Zucchet</b>, <b>Nicolas and Orvieto, Antonio},
	year = {2024},
	file = {Zucchet and Orvieto - 2024 - Recurrent neural networks vanishing and exploding.pdf:/Users/nicolas/Zotero/storage/CSKYBDYM/Zucchet and Orvieto - 2024 - Recurrent neural networks vanishing and exploding.pdf:application/pdf},
	selected={true},
	pdf={https://arxiv.org/pdf/2405.21064},
}

# 2023

@article{zucchetb_gated_2023,
	title = {Gated recurrent neural networks discover attention},
	journal = {arXiv preprint arXiv:2309.01775},
	author = {Zucchet*</b>, <b>Nicolas and Kobayashi*, Seijin and Akram*, Yassir and von Oswald, Johannes and Larcher, Maxime and Steger, Angelika and Sacramento, João},
	year = {2023},
  pdf={https://arxiv.org/pdf/2309.01775},
  thread={https://x.com/NicolasZucchet/status/1755925788648739276},
}

@article{von_oswald_uncovering_2023,
	title = {Uncovering mesa-optimization algorithms in transformers},
	journal = {arXiv preprint arXiv:2309.05858},
	author = {von Oswald*, Johannes and Niklasson*, Eyvind and Schlegel*, Maximilian and Kobayashi, Seijin and Zucchet</b>, <b>Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and Vladymyrov, Max and Pascanu, Razvan and Sacramento, João},
	year = {2023},
  pdf={https://arxiv.org/pdf/2309.05858},
  thread={https://x.com/oswaldjoh/status/1701873029100241241},
}

@inproceedings{zucchetb_online_2023,
	title = {Online learning of long-range dependencies},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zucchet*</b>, <b>Nicolas and Meier*, Robert and Schug*, Simon and Mujika, Asier and Sacramento, João},
	year = {2023},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
  selected={true},
  code={https://github.com/NicolasZucchet/Online-learning-LR-dependencies},
  pdf={online_learning_lr_dep.pdf},
  poster={poster_online_learning_long_range_dependencies.pdf},
  thread={https://x.com/NicolasZucchet/status/1733119036140249579},
}

# 2022

@inproceedings{meulemans_least-control_2022,
	title = {The least-control principle for local learning at equilibrium},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Meulemans*, Alexander and Zucchet*</b>, <b>Nicolas and Kobayashi*, Seijin and von Oswald, Johannes and Sacramento, João},
	year = {2022},
  selected={true},
  award={Oral and pre-selected for best paper award.},
  pdf={https://arxiv.org/pdf/2207.01332},
  code={https://github.com/seijin-kobayashi/least-control},
  poster={poster_lcp.pdf},
  thread={https://x.com/NicolasZucchet/status/1591041946856062976},
}

@inproceedings{benzing_random_2022,
	title = {Random initialisations performing above chance and how to find them},
	booktitle = {Annual {Workshop} on {Optimization} for {Machine} {Learning}},
	author = {Benzing, Frederik and Schug, Simon and Meier, Robert and von Oswald, Johannes and Akram, Yassir and Zucchet</b>, <b>Nicolas and Aitchison, Laurence and Steger, Angelika},
	year = {2022},
	keywords = {Computer Science - Machine Learning},
  pdf={https://arxiv.org/pdf/2209.07509},
  thread={https://x.com/ssmonsays/status/1571126507933929481},
}

@inproceedings{zucchetb_contrastive_2022,
	title = {A contrastive rule for meta-learning},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Zucchet*</b>, <b>Nicolas and Schug*, Simon and von Oswald*, Johannes and Zhao, Dominic and Sacramento, João},
	year = {2022},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
  pdf={https://arxiv.org/pdf/2104.01677},
  poster={poster_cml.png},
  thread={https://x.com/ssmonsays/status/1585955752854048768},
}

@article{zucchetb_beyond_2022,
	title = {Beyond backpropagation: bilevel optimization through implicit differentiation and equilibrium propagation},
	volume = {34},
	number = {12},
	journal = {Neural Computation},
	author = {Zucchet</b>, <b>Nicolas and Sacramento, João},
	year = {2022},
  pdf={https://arxiv.org/pdf/2205.03076},
}

# 2021

@inproceedings{von_oswald_learning_2021,
	title = {Learning where to learn: {Gradient} sparsity in meta and continual learning},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {von Oswald*, Johannes and Zhao*, Dominic and Kobayashi, Seijin and Schug, Simon and Caccia, Massimo and Zucchet</b>, <b>Nicolas and Sacramento, João},
	year = {2021},
  pdf={https://arxiv.org/pdf/2110.14402},
}
